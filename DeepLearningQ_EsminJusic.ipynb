{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOldoeuIKRe6Am5IyjfaCXK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EsminJ/textGeneration-/blob/main/DeepLearningQ_EsminJusic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWJPPGZJMEtc",
        "outputId": "0dde6638-edc5-4c89-a059-9301b480699e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training...\n",
            "Epoch 1/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - loss: 3.5507\n",
            "Epoch 2/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 3.0963\n",
            "Epoch 3/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 2.9982\n",
            "Epoch 4/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 2.9861\n",
            "Epoch 5/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 2.8875\n",
            "Epoch 6/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - loss: 2.9098\n",
            "Epoch 7/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 2.8494\n",
            "Epoch 8/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 2.8313\n",
            "Epoch 9/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 2.7102\n",
            "Epoch 10/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - loss: 2.6638\n",
            "Epoch 11/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 2.7031\n",
            "Epoch 12/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 2.5435\n",
            "Epoch 13/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 2.5313\n",
            "Epoch 14/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 2.3314\n",
            "Epoch 15/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 2.2353\n",
            "Epoch 16/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 2.1731\n",
            "Epoch 17/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - loss: 1.8844\n",
            "Epoch 18/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.7885\n",
            "Epoch 19/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - loss: 1.6100\n",
            "Epoch 20/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 1.4883\n",
            "Epoch 21/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 1.3797\n",
            "Epoch 22/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 1.0899\n",
            "Epoch 23/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 0.9260\n",
            "Epoch 24/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 0.7426\n",
            "Epoch 25/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - loss: 0.5763\n",
            "Epoch 26/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 0.4369\n",
            "Epoch 27/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 0.3440\n",
            "Epoch 28/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 0.2570\n",
            "Epoch 29/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - loss: 0.1779\n",
            "Epoch 30/30\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - loss: 0.1435\n",
            "To be, or not to be, that is the To be,  tiiiaan hhaaaao aaao,hhhhhtsshseed  oo ii mmn ouuuuggggeeesssaaaaoo  hees  hotteeeAA   o iiii  hhhmao aoi  aaouuuugetsgsdaaaoo   seee—  ooytllbeppppppp pii   iea— oyye———ollllppppppipc py  iie   yyee\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow . keras . models import Sequential\n",
        "from tensorflow . keras . layers import LSTM , Dense , Embedding\n",
        "import numpy as np\n",
        "\n",
        "# 1. Prepare text data\n",
        "text = \"To be, or not to be, that is the question: Whether 'tis nobler in the mind to suffer The slings and arrows of outrageous fortune, Or to take arms against a sea of troubles And by opposing end them. To die, to sleep— No more—and by a sleep to say we end The heart-ache and the thousand natural shocks That flesh is heir to. ’Tis a consummation Devoutly to be wish’d. To die, to sleep— To sleep, perchance to dream—ay, there’s the rub: For in that sleep of death what dreams may come, When we have shuffled off this mortal coil, Must give us pause.\"\n",
        "# Load your text corpus\n",
        "# Tokenize and create sequences\n",
        "unique_characters = sorted(list(set(text)))\n",
        "vocab_size = len(unique_characters)\n",
        "char_to_idx = {c: i for i, c in enumerate(unique_characters)}\n",
        "idx_to_char = {i: c for i, c in enumerate(unique_characters)}\n",
        "sequence_length = 40\n",
        "input_sequences = []\n",
        "target_chars = []\n",
        "step = 3\n",
        "for i in range(0, len(text) - sequence_length, step):\n",
        "    seq = text[i : i + sequence_length]\n",
        "    target = text[i + sequence_length]\n",
        "    input_sequences.append([char_to_idx[c] for c in seq])\n",
        "    target_chars.append(char_to_idx[target])\n",
        "\n",
        "x = np.array(input_sequences)\n",
        "y = tf.keras.utils.to_categorical(target_chars, num_classes=vocab_size)\n",
        "\n",
        "\n",
        "# 2. Build LSTM model\n",
        "vocab_size = len( unique_characters ) # or unique_words\n",
        "embedding_dim = 256\n",
        "lstm_units = 512\n",
        "\n",
        "model = Sequential ([\n",
        "Embedding ( vocab_size , embedding_dim ,\n",
        "input_length = sequence_length ) ,\n",
        "LSTM ( lstm_units , return_sequences = True ) ,\n",
        "LSTM ( lstm_units ) ,\n",
        "Dense ( vocab_size , activation ='softmax')\n",
        "])\n",
        "\n",
        "# 3. Train model\n",
        "model.compile (loss = 'categorical_crossentropy' ,\n",
        "optimizer ='adam')\n",
        "\n",
        "print(\"\\nTraining...\")\n",
        "history = model.fit(x, y,epochs=30, batch_size=32)\n",
        "\n",
        "# 4. Generate text\n",
        "def sample_with_temperature(probs, temperature=1.0):\n",
        "    probs = np.asarray(probs).astype(\"float64\")\n",
        "    probs = np.log(probs + 1e-8) / temperature\n",
        "    probs = np.exp(probs) / np.sum(np.exp(probs))\n",
        "    return np.random.choice(len(probs), p=probs)\n",
        "\n",
        "def generate_text ( seed_text , length =100 , temperature =1.0) :\n",
        "  seed = ''.join(c for c in seed_text if c in char_to_idx)\n",
        "  if len(seed) < sequence_length:\n",
        "      seed = (text[:sequence_length - len(seed)]) + seed\n",
        "\n",
        "  generated = seed\n",
        "\n",
        "  for _ in range(length):\n",
        "      recent = generated[-sequence_length:]\n",
        "      x_pred = np.array([[char_to_idx[c] for c in recent]])\n",
        "      preds = model.predict(x_pred, verbose=0)[0]\n",
        "      next_idx = sample_with_temperature(preds, temperature)\n",
        "      next_char = idx_to_char[next_idx]\n",
        "      generated += next_char\n",
        "\n",
        "  return generated\n",
        "# Your generation code here\n",
        "print(generate_text(\"To be, \", length=200, temperature=0.7))\n"
      ]
    }
  ]
}